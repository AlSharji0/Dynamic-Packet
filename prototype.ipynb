{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicTargetEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(DynamicTargetEnv, self).__init__()\n",
    "        \n",
    "        # State constraints\n",
    "        self.feature_ranges = [(0, 5), (0, 1), (0, 5)]  # Ranges for x1, x2, x3\n",
    "        \n",
    "        # Define action space: 2 actions per feature (increment, decrement)\n",
    "        self.action_space = spaces.Discrete(6)\n",
    "        \n",
    "        # Define observation space: Each feature has its own range\n",
    "        self.observation_space = spaces.MultiDiscrete([r[1] - r[0] + 1 for r in self.feature_ranges])\n",
    "        \n",
    "        self.state = None\n",
    "        self.steps = 0\n",
    "        self.max_steps = 1000  # Prevent infinite loops\n",
    "        \n",
    "        # Initialize the target state\n",
    "        self.target_state = self._generate_target_state()\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        # Initialize state to the minimum values for each feature\n",
    "        self.state = np.array([r[0] for r in self.feature_ranges], dtype=np.int32)\n",
    "        self.steps = 0\n",
    "        return self.state, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        self.steps += 1\n",
    "        \n",
    "        # Map action to feature and operation (increment or decrement)\n",
    "        feature_index = action // 2\n",
    "        increment = 1 if action % 2 == 0 else -1\n",
    "        \n",
    "        # Update the state within bounds\n",
    "        feature_min, feature_max = self.feature_ranges[feature_index]\n",
    "        self.state[feature_index] = np.clip(\n",
    "            self.state[feature_index] + increment, feature_min, feature_max\n",
    "        )\n",
    "\n",
    "        # Check if the current state matches the target state\n",
    "        if np.array_equal(self.state, self.target_state):\n",
    "            reward = 100.0  # Large reward for success\n",
    "            done = True\n",
    "            print(f\"Success! Target state achieved: {self.target_state}\")\n",
    "            self.target_state = self._generate_target_state()  # Generate a new target state\n",
    "            print(f\"New target state: {self.target_state}\")\n",
    "        else:\n",
    "            reward = -0.1  # Small penalty for each step\n",
    "            done = False\n",
    "\n",
    "        # End the episode if max steps are reached\n",
    "        if self.steps >= self.max_steps:\n",
    "            done = True\n",
    "\n",
    "        return self.state, reward, done, False, {}\n",
    "\n",
    "    def render(self):\n",
    "        print(f\"State: {self.state}, Target: {self.target_state}\")\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "    def _generate_target_state(self):\n",
    "        \"\"\"\n",
    "        Randomly generates a new target state within the feature ranges.\n",
    "        \"\"\"\n",
    "        return np.array([np.random.randint(low=r[0], high=r[1] + 1) for r in self.feature_ranges])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial State: [0 0 0]\n",
      "Action: 5, State: [0 0 0], Reward: -0.1\n",
      "Action: 1, State: [0 0 0], Reward: -0.1\n",
      "Action: 1, State: [0 0 0], Reward: -0.1\n",
      "Action: 4, State: [0 0 1], Reward: -0.1\n",
      "Action: 1, State: [0 0 1], Reward: -0.1\n",
      "Action: 1, State: [0 0 1], Reward: -0.1\n",
      "Action: 3, State: [0 0 1], Reward: -0.1\n",
      "Action: 1, State: [0 0 1], Reward: -0.1\n",
      "Action: 0, State: [1 0 1], Reward: -0.1\n",
      "Action: 3, State: [1 0 1], Reward: -0.1\n",
      "Action: 1, State: [0 0 1], Reward: -0.1\n",
      "Action: 4, State: [0 0 2], Reward: -0.1\n",
      "Action: 1, State: [0 0 2], Reward: -0.1\n",
      "Action: 0, State: [1 0 2], Reward: -0.1\n",
      "Success! Target state achieved: [1 0 3]\n",
      "New target state: [5 0 2]\n",
      "Action: 4, State: [1 0 3], Reward: 100.0\n",
      "Terminal state reached!\n"
     ]
    }
   ],
   "source": [
    "env = DynamicTargetEnv()\n",
    "\n",
    "state, _ = env.reset()\n",
    "print(f\"Initial State: {state}\")\n",
    "\n",
    "for _ in range(200):  # Simulate up to 20 steps\n",
    "    action = env.action_space.sample()  # Random action\n",
    "    state, reward, done, truncated, _ = env.step(action)\n",
    "    print(f\"Action: {action}, State: {state}, Reward: {reward}\")\n",
    "    if done:\n",
    "        print(\"Terminal state reached!\")\n",
    "        break\n",
    "\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
